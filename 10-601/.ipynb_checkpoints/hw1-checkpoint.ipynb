{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"hw1.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1e1b2709208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"hw1.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Probability Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $$P(A,B) = P(A|B)P(B) = P(B|A)P(A).$$\n",
    "A re-arrangement of terms on the right gives the desired equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Contingencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $P(A = \\triangle) = P(A=\\triangle, B=\\triangle) + P(A=\\triangle, B=\\square) = \\frac{15}{117}$.\n",
    "2. $P(A=\\triangle, B=\\square) = \\frac{3}{117}$.\n",
    "3. $P(A=\\triangle \\text{ OR } B=\\square) = \\frac{15}{117} + \\frac{5}{117} = \\frac{20}{117}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $$P(A,B,C) = P(A|B,C)P(B|C)P(C).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Total Probability and Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We have $$P(X=1) = P(X=1|Y=1,Z=1)P(Y=1|Z=1)P(Z=1) + P(X=1|Y=1,Z=0)P(Y=1|Z=0)P(Z=0) + P(X=1|Y=0)P(Y=0|Z)P(Z)$$\n",
    "Noting that $Y,Z$ are independent this reduces to \n",
    "\\begin{align}\n",
    "P(X=1) &= P(X=1|Y=1,Z=1)P(Y=1)P(Z=1) + P(X=1|Y=1,Z=0)P(Y=1)P(Z=0) + P(X=1|Y=0)P(Y=0) \\\\ &= (0.6)(0.9)(0.8) + (0.1)(0.9)(0.2) + (0.2)(0.1) \\\\ &= 0.47\n",
    "\\end{align}\n",
    "2. The expected value of $Y$ is $$E[Y] = 1 \\cdot P(Y=1) + 0 \\cdot P(Y=0) = 0.9$$\n",
    "3. With the new specifications, the expected value of $Y$ is $$E[Y] = 115 \\cdot P(Y=115) + 20 \\cdot P(Y=20) = 105.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Model Complexity and Data Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since $Y=1$ with probability $\\theta$ if the boolean expression is $1$, we have $$P(Y=1| (X_1 \\lor (\\lnot X_1 \\land X_2 \\land X_6)) = 1) = \\theta.$$\n",
    "2. By similar logic, we can say that $$P(Y=1| (\\lnot(X_1 \\lor (\\lnot X_1 \\land X_2 \\land X_6)) = 1)) = 1-\\theta.$$\n",
    "3. No. If $X_2 = 0$, the right disjunct (with the ands) will be false so since the $X_i$ each are uniformly distributed over $\\{0,1\\}$ there is a 50% chance that $X_1 = 1$ which would allow for $Y=1$. On the other hand, if $X_2 = 1$ then the right disjunct could equal $1$, so there's a higher chance of $Y=1$.\n",
    "4. Yes. Notice that $X_4$ is irrelevant to the outcome.\n",
    "5. The accuracy should be $\\theta$, which controls the probability that $y$ is the actual correct thing.\n",
    "6. This should also be $\\theta$ since things average out in the limit (?)\n",
    "7. The depth should be __$3$__ since there were $3$ variables used to learn the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 MLE and MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Maximum Likelihood Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Since we have $n$ Boolean values dran __independently__, the likelihood is $$L(\\theta) = P(X_1,...,X_n|\\theta) = P(X_1|\\theta)P(X_2|\\theta)...P(X_n|\\theta) = (\\theta)^{n_1}(1-\\theta)^{n_0},$$ where $n_1$ is the number of values that are $1$ and $n_0$ is the number that turned out to be $0$.\n",
    "2. ![MLE by Theta](MLE_theta.jpg)\n",
    "3. Our value of $\\theta$ that maximizes the likelihood on this graph appears to be a bit below 0.7. If we compare this to what we should get for MLE, or $$\\theta^{MLE} = \\frac{\\sum_i X_i}{n} = \\frac{6(1) + 3(0)}{9} = 0.66,$$ the graph agrees.\n",
    "4. The graph for 2 heads and one tail: ![MLE 2 heads](MLE_2heads.jpg)\n",
    "And the graph for 40 heads and 20 tails: ![MLE 40 heads](MLE_40heads.jpg)\n",
    "5. The proportion of heads to tails is the same in each case, so as we'd expect the optimal $\\theta$ is the same in each case. Sensibly, the likelihood curve for the 40 heads case is the most concentrated around $\\theta^{MLE}$, while the curve for 2 heads is the least concentrated since we have the least amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ![beta plot](beta33_plot.jpg)\n",
    "2. For this, we plot the likelihood function times our prior, or $$P(X_1,..,X_6|\\theta)P(\\theta) = \\theta^6(1-\\theta)^3 \\left( \\frac{\\theta^{\\beta_H - 1}(1-\\theta)^{\\beta_T - 1}}{0.0333} \\right)$$ to get the plot below: ![map_beta](map_beta.jpg)\n",
    "The optimal value of beta is __a little smaller__ than the one from MLE, influenced by the prior belief and so shrunken towards the middle.\n",
    "3. __Yes__. If we are to use the data with $n_H = 2$ and $n_T = 1$ with a prior of Beta(5,3) then we'll end up with a posterior of Beta(7,4), i.e. proportional to $\\theta^{7-1}(1-\\theta)^{4-1}$ which is proportional to the likelihood function for 6 heads and 3 tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
