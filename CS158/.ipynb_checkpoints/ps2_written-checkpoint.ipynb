{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"ps2_exercises.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f59c66b7f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"ps2_exercises.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Trees Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any 1-leaf decision tree either predicts yes or no on every single input. The best 1-leaf decision tree should answer yes for everything because the only times when it answers no are when all three of $X_1$, $X_2$ and $X_3$ are $0$. If $n=4$ this occurs twice. If $n=5$ this occurs $4$ times so in general the number of wrong answers will be $2^{n-3}$. So this occurs $2^{n-3}/2^n = \\frac{1}{8}$ of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a split that reduces the number of mistakes by at least one. If we split on a feature, then the only features we can decide to split on are whether a single feature $X_i$ is set to zero or one. If we were to split on any of these features and answere yes in one case and no in the other, the number of mistakes would increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we split on $X_i$ with $i \\geq 4$ then the data will be split so that the proportion of ones in each leaf will be $\\frac{7}{8}$. In both leaves $1$ will be predicted which is the same.\n",
    "If we split on $X_1$, $X_2$, or $X_3$ it's again the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $$H(Y) = -\\sum_{y \\in R_Y}p(y)\\log \\frac{1}{p(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the probability that $y=0$ is the probaiblity that $X_1$,$X_2$ and $X_3$ are all $0$ which is $\\frac{1}{8}$ and so the probability that $y=1$ is $\\frac{7}{8}$ giving us an entropy of $$H(Y) = 0.377$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. If we split on $X_1$, $X_2$ or $X_3$ then the entropy is $$\\frac{1}{2}(0) + \\frac{1}{2} \\left[ \\frac{1}{4}\\log(4) + \\frac{3}{4}\\log \\left( \\frac{4}{3} \\right) \\right] = 0.406.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Entropy and Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write this out, $$H(S) = -\\left( \\frac{p}{p+n}\\log\\frac{p+n}{p} + \\frac{n}{p+n}\\log\\frac{p+n}{n} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lecture defined the info gain of $X$ and $Y$ as the mutual information $$I(X;Y) = H(X) - H(X|Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the ratio is the same, then the gain is $$\\text{Gain} = B\\left(\\frac{p}{p+n}\\right) -\\sum_{k=1}^d \\frac{p_k+n_k}{p+n}B\\left( \\frac{p_k}{p_k+n_k}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $p = \\sum_k p_k$ and $n = \\sum_k n_k$ and since ratios are the same $p_k / (p_k+n_k) = p / (p+n)$ for all $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have $$\\text{Gain} = B\\left(\\frac{p}{p+n}\\right) -\\frac{p+n}{p+n}B\\left( \\frac{p}{p+n}\\right) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 K-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat here is that __a point can be its own neighbor__ so the value of $k$ that minimizes the error for the training set is __$k=0$__ which gives us an error of $0$. This isn't reasonable for a test set because a test point doesn't come with a label and here we're basically cheating by using the training label to classify the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this is that values of $k$ that are extremely small lead to __overfitting__. The highest value ($k=13$) misclassifies every data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how LOOCV works: we train on $n-1$ examples and validate on the remaining one, doing so $n$ times where each training example gets a turn at being left out for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $k=5$ minimizes the error using LOOCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
