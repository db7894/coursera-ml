{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"hw5.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1d05f6f8a20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"hw5.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[c]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\sigma = 0.1$ and $d=8$, we want to solve the following inequality for $N$: $$ \\sigma^2 \\left( 1 - \\dfrac{d+1}{N} \\right) \\geq 0.008 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging things in we get $$ (0.1)^2 \\left( 1 - \\dfrac{9}{N} \\right) \\geq 0.008,$$\n",
    "where dividing out the $\\sigma$ value gives $$ \\left( 1 - \\dfrac{9}{N} \\right) \\geq 0.8, $$\n",
    "and by moving terms around more we have $$ 0.2 \\geq \\dfrac{9}{N}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, multiplying both sides by $N$ and dividing through by $0.2$ gives us the solution $$ N \\geq 45. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the answer choices, the smallest $N$ that gives us the desired expected value for $E_{in}$ is $N = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the basic equation for a hyperbola centered at $(0,0)$ and see if that helps (in terms of $x_1$ and $x_2$): $$ \\dfrac{x_1^2}{a} - \\dfrac{x_2^2}{b} = 1, $$ where the distance between the vertices is $2a$ and distance between foci is $2c$ where $c^2 = a^2+b^2$, _not that we actually care about any of that information_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the decision boundary assigns a positive value $+1$ to a training example $(1,x_1,x_2)$ if it falls __in between the hyperbola boundaries__. \n",
    "\n",
    "Let's simplify things further and get rid of the annoying $a$ and $b$ to yield the equation $$ x_1^2 - x_2^2 = 1, $$ which tells us we want something like $$ x_1^2 - x_2^2 < 1. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the $\\mathcal{Z}$-space, we have $ z_1 = x_1^2 $ and $ z_2 = x_2^2 $. Our linear hypothesis looks like $$ \\tilde{w}^T\\boldsymbol{z} = \\tilde{w}_0z_0 + \\tilde{w}_1z_1 + \\tilde{w}_2z_2 = \\tilde{w}_0 + \\tilde{w}_1x_1^2 + \\tilde{w}_2x_2^2. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want $ \\tilde{w}_1 < 0 $ and $ \\tilde{w}_2 > 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[c]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed space has $15$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the partial derivative $ \\frac{\\partial E}{\\partial u} $ of $$ E(u,v) = (ue^v - 2ve^{-u})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the chain rule,we get $$ \\dfrac{\\partial E}{\\partial u} = 2(ue^v - 2ve^{-u})(e^v + 2ve^{-u}), $$ noting that the $u$ does not come down from the exponential because we differentiated with respect to $u$ (just as $ \\frac{d}{du}e^u = e^u $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want the partial derivative of $E$ with respect to $v$: $$ \\dfrac{\\partial E}{\\partial v} = 2(ue^v - 2ve^{-u})(ue^v - 2e^{-u}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientError(u, v):\n",
    "    return (u*np.exp(v) - 2*v*np.exp(-u))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dE_du(u, v):\n",
    "    \"\"\"\n",
    "    returns the partial derivative of E(u,v) w/r/t u\n",
    "    \"\"\"\n",
    "    return 2 * (u * np.exp(v) - 2 * v * np.exp(-u)) * (np.exp(v) + 2 * v * np.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dE_dv(u, v):\n",
    "    \"\"\"\n",
    "    returns the partial derivative of E(u,v) w/r/t v\n",
    "    \"\"\"\n",
    "    return 2 * (u * np.exp(v) - 2 * v * np.exp(-u)) * (u * np.exp(v) - 2 * np.exp(-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateWeights(weights, nabla):\n",
    "    '''\n",
    "    assuming there are just 2 weights we care about\n",
    "    new_weight = old_weight - (learning rate) * (partial derivative w.r.t. u or v)\n",
    "    '''\n",
    "    u = weights[0]\n",
    "    v = weights[1]\n",
    "    \n",
    "    # perform update\n",
    "    weights[0] -= nabla * dE_du(u,v)\n",
    "    weights[1] -= nabla * dE_dv(u,v)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q5():\n",
    "    nabla = 0.1 # Lr given in problem\n",
    "    thresh = 10 ** -14 # thresh we want error to get below\n",
    "    weights = [1.0,1.0] # initial weight values\n",
    "    num_iters = 0\n",
    "    \n",
    "    while True:\n",
    "        error = gradientError(weights[0], weights[1]) # calculate current error\n",
    "        num_iters += 1\n",
    "        \n",
    "        if error < thresh or num_iters > 10000:\n",
    "            print(\"num iterations: \" + str(num_iters))\n",
    "            break\n",
    "        else:\n",
    "            weights = updateWeights(weights, nabla) # perform GD\n",
    "            \n",
    "    print(\"Error: \" + str(gradientError(weights[0], weights[1])))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num iterations: 11\n",
      "Error: 1.20868339442e-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.044736290397782069, 0.023958714099141746]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the answer choices, $10$ is the closest to the actual number of iterations we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We returned the weights above so we can just use the values we got, which are closest to the answer pair $[0.045, 0.024]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[a]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want methods to update u,v separately so let's write those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateU(weights, nabla):\n",
    "    weights[0] -= nabla # since u is the first coord we update weights[0]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateV(weights, nabla):\n",
    "    weights[1] -= nabla\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q7():\n",
    "    nabla = 0.1\n",
    "    weights = [1.0, 1.0]\n",
    "    num_iters = 15\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        weights = updateU(weights, nabla) # update u first\n",
    "        weights = updateV(weights, nabla) # then update v\n",
    "        \n",
    "    print(\"error: \" + str(gradientError(weights[0], weights[1])))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1.81025168875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.4999999999999999, -0.4999999999999999]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error is roughly 1.81, which is closest to the answer value $10^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy over some utility functions from HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generatePoints(numberOfPoints):\n",
    "    x1 = random.uniform(-1, 1)\n",
    "    y1 = random.uniform(-1, 1)\n",
    "    x2 = random.uniform(-1, 1)\n",
    "    y2 = random.uniform(-1, 1)\n",
    "    points = []\n",
    "\n",
    "    for i in range (numberOfPoints):\n",
    "        x = random.uniform (-1, 1)\n",
    "        y = random.uniform (-1, 1)\n",
    "        points.append([1, x, y, targetFunction(x1, y1, x2, y2, x, y)]) # add 1/-1 indicator to the end of each point list\n",
    "    return x1, y1, x2, y2, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFunction(x1,y1,x2,y2,x3,y3):\n",
    "    u = (x2-x1)*(y3-y1) - (y2-y1)*(x3-x1)\n",
    "    if u >= 0:\n",
    "        return 1\n",
    "    elif u < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochasticGradient(weights, point):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    return (np.array(point[:3]) * point[3])/(1.0 + np.exp(point[3]*np.dot(weights, point[:3]))) * - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochasticLogisticRegression(threshold, weights, points, nabla):\n",
    "    increment = 1.0\n",
    "    iterations = 0\n",
    "    \n",
    "    while np.any(increment >= threshold):\n",
    "        iterations += 1\n",
    "        random.shuffle(points)\n",
    "        oldWeights = list(weights)\n",
    "        \n",
    "        # perform gradient descent on the weights\n",
    "        for point in points:\n",
    "            grad = stochasticGradient(weights, point)\n",
    "            weights -= nabla * grad # decrement using LR\n",
    "            \n",
    "        # we store the increment at the end of each point, and will eventually stop when increment is below 0.01 (threshold)\n",
    "        increment = np.abs(oldWeights - weights) # we are interested in when w(t) - w(t-1) < 0.01 so we want this value.\n",
    "        \n",
    "    return weights, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropy(weights, points):\n",
    "    \"\"\"\n",
    "    cross-entropy cost function\n",
    "    \n",
    "    for each point (x_n,y_n) we know point[3] = y_n, while point[:3] = x_n because of how defined.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for point in points:\n",
    "        vals.append(np.log(1.0 + np.exp(- point[3] * np.dot(weights, point[:3]))))\n",
    "        \n",
    "    return np.mean(vals) # the 1/N term in the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q8(numTrials, numPoints):\n",
    "    \"\"\"\n",
    "    for this question we will do 100 trials with N=100 points.\n",
    "    \"\"\"\n",
    "    eouts = []\n",
    "    iters = []\n",
    "    thresh = 0.01\n",
    "    nabla = 0.01\n",
    "    numPoints = 1000\n",
    "    \n",
    "    for i in range(numTrials):\n",
    "        weights = np.array([0.0,0.0,0.0]) # init weights\n",
    "        x1, y1, x2, y2, points = generatePoints(numPoints)\n",
    "        weights, iterations = stochasticLogisticRegression(thresh, weights, points, nabla)\n",
    "        errorCt = 0\n",
    "        \n",
    "        # generate more points to get e_out\n",
    "        x = []\n",
    "        for i in range(numPoints):\n",
    "            x_ = random.uniform(-1,1)\n",
    "            y_ = random.uniform(-1,1)\n",
    "            x.append([1, x_, y_, targetFunction(x1,y1,x2,y2,x_,y_)]) # same form as usual\n",
    "            \n",
    "        eouts.append(crossEntropy(weights, x)) # calculate crossentropy of trained weights on new points\n",
    "        iters.append(iterations)\n",
    "        \n",
    "    return np.mean(eouts), np.mean(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.035797335507102269, 692.15999999999997)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q8(100,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above run, the average $E_{out}$ is $0.0357$.\n",
    "\n",
    "At the moment, the answers for this question and question 9 do not match with the expected responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the above run--the mean number of iterations was roughly $692$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is asking which error measure, if used with SGD, would produce the exact same weight update as with the PLA algorithm we learned at the beginning of the course. For reference, recall that to do this we would pick a _misclassified_ points $\\text{sign}(w^T\\boldsymbol{x}_n) \\neq y_n$ and update the weight vector $$ \\boldsymbol{w} \\leftarrow \\boldsymbol{w} + y_nx_n. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient descent, we perform the update $$ \\boldsymbol{w} \\leftarrow \\boldsymbol{w} - \\nabla E_{in}(\\boldsymbol{w}(0)). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the only answer with expected behavior is $$ -\\min(0, y_n\\boldsymbol{w}^Tx_n). $$\n",
    "As with the PLA algorithm, $\\boldsymbol{w}$ is not updated in the case that our point is correctly classified (the error function is 0), but if not then we differentiate $ y_n\\boldsymbol{w}^Tx_n $ with respect to $\\boldsymbol{w}$ which gives us an update of $y_nx_n$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
