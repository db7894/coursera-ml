{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"final.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f616096b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"final.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, since $Q=10$, the order of the highest term in the transform will be $10$. The book tells us that the feature transform $\\Phi_Q$ maps a two-dimensional vector $\\textbf{x}$ to $$ \\tilde{d} = \\dfrac{Q(Q+3)}{2} $$ dimensions. Plugging in $Q=10$, we have $$ \\tilde{d} = 65. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately eliminate choice __[a]__ since if $\\mathcal{H}$ is a singleton hypothesis set then any hypothesis trained on any dataset will be the same $g \\in \\mathcal{H}$, meaning $\\bar{g}$ is the same as that single hypothesis in $\\mathcal{H}$. \n",
    "\n",
    "Choice __[b]__ is also out because if $\\mathcal{H}$ is the set of constant, real-valued hypotheses, then the average $\\bar{g}$ of any number of hypotheses $g^{(\\mathcal{D})} \\in \\mathcal{H}$ will also be in $\\mathcal{H}$, because the average of any set of real-valued constants is also a real-valued constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine choice __[c]__. Linear regression will output a hypothesis of the form $ax + b$ defined by the weights learned. If we think about this numerically or graphically, we can see that the average of any number of lines with real-valued parameters will also be a line that has real-valued parameters, so we can eliminate this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression outputs a hypothesis of the form $$ h(\\textbf{x}) = \\theta(\\textbf{w}^T\\textbf{x}), $$ where $\\theta(s) = \\frac{e^s}{1 + e^s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine overfitting, we want to compare the values of $(E_{out} - E_{in})$ among different hypotheses, which can tell us if we are encountering this issue or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, deterministic noise depends on the hypothesis set _and_ the target function, because it tracks the hypothesis set's inability to fully approximate the target function (not powerful enough).\n",
    "\n",
    "By contrast, stochastic noise is entirely dependent on the target distribution, because a dataset where the data points have noise is generated by the target distribution and is not affected in any way by the hypothesis set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[a]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On page 130 of LfD, it says if $\\textbf{w}_{\\text{lin}}^T\\textbf{w}_{\\text{lin}} \\leq C$, then $\\textbf{w}_{\\text{reg}} = \\textbf{w}_{\\text{lin}}$ because $\\textbf{w}_{\\text{lin}} \\in \\mathcal{H}(C)$, where $$ \\mathcal{H}(C) = \\{ h \\ \\vert \\ h(x) = \\textbf{w}^T\\textbf{z}, \\textbf{w}^T\\textbf{w} \\leq C \\}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[b]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed how to translate a soft-order constraint like the basic regularization one into an augmented error as follows: $$ E_{aug}(\\textbf{w}) = E_{in}(\\textbf{w}) + \\lambda\\textbf{w}^T\\textbf{w}, $$ where $\\lambda \\geq 0$ is now a free parameter that we can choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = 'features.train'\n",
    "test_file = 'features.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):    \n",
    "    with open(filename, \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                y, x1, x2 = line.split()\n",
    "                data.append([ int(float(y)), float(x1), float(x2) ])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = load_data(train_file)\n",
    "data_test = load_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_vs_all(k, data):\n",
    "    \"\"\"\n",
    "    k-vs-all: recall in 1-vs-all one digit has class +1 w/ rest -1.\n",
    "    k is just number k--so basically if the data pt has value k (e.g. 1, etc.) we label it with +1 in new y\n",
    "    \"\"\"\n",
    "    y = [data[i][0] for i in range(len(data))]\n",
    "    X = [[data[i][1], data[i][2]] for i in range(len(data))]\n",
    "    \n",
    "    y_new = []\n",
    "    ct = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == k:\n",
    "            y_new.append(1.0)\n",
    "            ct += 1\n",
    "        else:\n",
    "            y_new.append(-1.0)\n",
    "            \n",
    "    return y_new, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegression(X, y):        \n",
    "    # convert to np\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_inv = np.linalg.pinv(X)\n",
    "    \n",
    "    # use w = X_inv * y one-shot learning\n",
    "    return X_inv.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularizedLinearRegression(samplePoints, l):\n",
    "    \"\"\"\n",
    "    perform LR with regularization, where l is lambda\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    y_location = len(samplePoints[0]) -1 # y's location is assumed to be the last element in the list\n",
    "    \n",
    "    # construct X, split y vals\n",
    "    for point in samplePoints:\n",
    "        X.append(np.array(point[:y_location]))\n",
    "        y.append(point[y_location])\n",
    "        \n",
    "    weights = linearRegression(samplePoints) # get weights to use for regularization\n",
    "    X = np.array(X)\n",
    "    \n",
    "    # get the regularized form of the inverse, taking the pseudo-inv of X + lambda/N * wTw\n",
    "    X_regInv = np.linalg.pinv(X + np.array(l / len(samplePoints) * weights.dot(weights)))\n",
    "    \n",
    "    return X_regInv.dot(y) # again using one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regularizedLinearRegression2(X, y, l):\n",
    "    \"\"\"\n",
    "    perform LR with regularization, where l is lambda\n",
    "    \"\"\"\n",
    "    weights = linearRegression(X, y) # get weights to use for regularization\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # get the regularized form of the inverse, taking the pseudo-inv of X + lambda/N * wTw\n",
    "    X_regInv = np.linalg.pinv(X + np.array(l / len(X) * weights.dot(weights)))\n",
    "    \n",
    "    return X_regInv.dot(y) # again using one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def X_reshape(X):\n",
    "    num_ex = X.shape[0]\n",
    "    X_res = np.c_[np.ones(num_ex), X]\n",
    "\n",
    "    return X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(weights, X):\n",
    "    real_X = X_reshape(X)\n",
    "    cur_h = np.matmul(real_X, weights)\n",
    "    return cur_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_error(weights, X, Y):\n",
    "    nb_ex = X.shape[0]\n",
    "    predicted = np.sign(predict(weights, X))\n",
    "    num_incorrect = np.sum(np.not_equal(predicted, np.sign(Y)))\n",
    "    prop_incorrect = float(num_incorrect)/float(num_ex)\n",
    "    return prop_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_reshape_nlt(X):\n",
    "    num_ex = X.shape[0]\n",
    "    #nlt = (1, x1, x2, x1x2, x1^2, x2^2)\n",
    "    X_mult = np.prod(X, axis=1)\n",
    "    X_res = np.c_[np.ones(num_ex), X, X_mult, np.square(X)]\n",
    "    return X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (7291,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b40c00e7f12f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregularizedLinearRegression2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mE_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mE_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-9b334907f5ae>\u001b[0m in \u001b[0;36mcalc_error\u001b[1;34m(weights, X, Y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnb_ex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnum_incorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprop_incorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_incorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_ex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-b32f1d03adad>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(weights, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mreal_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcur_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcur_h\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (7291,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "for val in range(10):\n",
    "    y_train, X_train = k_vs_all(val, data_train)\n",
    "    y_test,  X_test = k_vs_all(val, data_test)\n",
    "    y_train, X_train = np.array(y_train), np.array(X_train)\n",
    "    y_test, X_test = np.array(y_test), np.array(X_test)\n",
    "    \n",
    "    Xtrain_nlt = X_reshape_nlt(X_train)\n",
    "    Xtest_nlt = X_reshape_nlt(X_test)\n",
    "    \n",
    "    w = regularizedLinearRegression2(X_train, y_train, 1)\n",
    "    \n",
    "    E_in = calc_error(w, X_train, y_train)\n",
    "    E_out = calc_error(w, X_test, y_test)\n",
    "    \n",
    "    print(\"___ %d vs all ___\" % val)\n",
    "    print(\"no transform: e_in = %f, e_out = %f\" % (E_in, E_out))\n",
    "    \n",
    "    w_nlt = regularizedLinearRegression2(Xtrain_nlt, y_train, 1)\n",
    "    Ein_nlt = calc_error(w_nlt, X_train, y_train)\n",
    "    Eout_nlt = calc_error(w_nlt, X_test, y_test)\n",
    "    \n",
    "    print(\"transform: e_in = %f, e_out = %f\" % (Ein_nlt, Eout_nlt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
