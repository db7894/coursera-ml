{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"hw2.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1cdd9e3a400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"hw2.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Coin:\n",
    "    \"\"\"\n",
    "    a simple coin class that gives us a utility to flip a single coin a given number of times (fair or unfair).\n",
    "    \"\"\"\n",
    "    def __init__(self, side=None):\n",
    "        self.side = side\n",
    "        self.heads_flipped = 0 # number of heads flipped in this coin's history\n",
    "        self.tails_flipped = 0\n",
    "        self.frac_heads = 0 # fraction of heads depending on num_flips\n",
    "    def flip(self, thresh=0.5, num_flips=1, track_flips=False):\n",
    "        \"\"\"\n",
    "        flip the coin, 50% probability of self.side changing to heads/tails or whatever thresh is\n",
    "        \"\"\"\n",
    "        self.heads_flipped = 0 # reset every time we do a number of flips.\n",
    "        self.tails_flipped = 0\n",
    "        \n",
    "        num_heads = 0\n",
    "        num_tails = 0\n",
    "        \n",
    "        if track_flips:\n",
    "            flips_so_far = []\n",
    "            \n",
    "        for i in range(num_flips):\n",
    "            val = np.random.uniform(0,1)\n",
    "            if val >= 0.5:\n",
    "                self.side = \"heads\"\n",
    "                self.heads_flipped += 1\n",
    "                num_heads += 1\n",
    "            else:\n",
    "                self.side = \"tails\"\n",
    "                self.tails_flipped += 1\n",
    "                num_tails += 1\n",
    "                \n",
    "            # print(\"Flipped \" + str(self.side))\n",
    "                \n",
    "            if track_flips:\n",
    "                flips_so_far.append(self.side) #append this flip\n",
    "        \n",
    "        self.frac_heads = self.heads_flipped / num_flips # store this fraction.\n",
    "        \n",
    "        if track_flips:\n",
    "            return num_heads, num_tails, flips_so_far\n",
    "        \n",
    "        return num_heads, num_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Coin()\n",
    "heads, tails = c.flip(num_flips=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_coin_objects(num_coins, num_flips):\n",
    "    \"\"\"\n",
    "    simulates the flipping of a number of coins--flips num_coins coins, flipping each for num_flips flips\n",
    "    \"\"\"\n",
    "    coins = []\n",
    "    for i in range(num_coins):\n",
    "        c = Coin()\n",
    "        c.flip(num_flips = num_flips)\n",
    "        coins.append(c) # so we can retrieve properties of individual coins.\n",
    "        # heads, tails = c.heads_flipped, c.tails_flipped # get number of heads/tails\n",
    "        \n",
    "    return coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flipped_coins = flip_coins(5,10) # test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = flipped_coins[0]\n",
    "c.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = min(flipped_coins, key=attrgetter('heads_flipped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for coin in flipped_coins:\n",
    "    print(coin.heads_flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so this appears to work--we have flip_coins which will create an array of coins which have properties we can access. Now we need a function to run the full experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_coins(num_coins, num_flips, thresh=0.5):\n",
    "    head_freqs = []\n",
    "    for i in range(num_coins):\n",
    "        num_heads = 0\n",
    "        \n",
    "        for j in range(num_flips):\n",
    "            val = np.random.uniform(0,1)\n",
    "            if val >= 0.5:\n",
    "                num_heads += 1 # else don't do anything.\n",
    "            \n",
    "        heads_frac = num_heads / num_flips\n",
    "        head_freqs.append(heads_frac)\n",
    "        \n",
    "    return head_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(num_coins, num_flips, num_runs):\n",
    "    \"\"\"\n",
    "    runs an experiment of flipping a certain number of coins\n",
    "    \"\"\"\n",
    "    v_one_avg = 0\n",
    "    v_rand_avg = 0\n",
    "    v_min_avg = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        flipped_coins = flip_coins(num_coins, num_flips) # get an array of coins\n",
    "        \n",
    "        # we have the direct values of frequency of heads so we can just get the v's.\n",
    "        v_one = flipped_coins[0] # first coin\n",
    "        v_rand = random.choice(flipped_coins) # random coin\n",
    "        v_min = min(flipped_coins) # coin w/ min number heads flipped\n",
    "        \n",
    "        v_one_avg += v_one\n",
    "        v_rand_avg += v_rand\n",
    "        v_min_avg += v_min\n",
    "        \n",
    "    v_one_avg /= num_runs\n",
    "    v_rand_avg /= num_runs\n",
    "    v_min_avg /= num_runs\n",
    "    \n",
    "    return v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment_object(num_coins, num_flips, num_runs):\n",
    "    \"\"\"\n",
    "    runs an experiment of flipping a certain number of coins (object version)\n",
    "    \"\"\"\n",
    "    v_one_avg = 0\n",
    "    v_rand_avg = 0\n",
    "    v_min_avg = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        flipped_coins = flip_coins(num_coins, num_flips) # get an array of coins\n",
    "        \n",
    "        c_one = flipped_coins[0] # first coin\n",
    "        c_rand = random.choice(flipped_coins) # random coin\n",
    "        c_min = min(flipped_coins, key=attrgetter('heads_flipped')) # coin w/ min number heads flipped\n",
    "        \n",
    "        v_one = c_one.frac_heads\n",
    "        v_rand = c_rand.frac_heads\n",
    "        v_min = c_min.frac_heads\n",
    "        \n",
    "        v_one_avg += v_one\n",
    "        v_rand_avg += v_rand\n",
    "        v_min_avg += v_min\n",
    "        \n",
    "    v_one_avg /= num_runs\n",
    "    v_rand_avg /= num_runs\n",
    "    v_min_avg /= num_runs\n",
    "    \n",
    "    return v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the experiment, and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one_avg, v_rand_avg, v_min_avg = run_experiment(5,10,1) # test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.5, 0.4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run 100,000 times, flipping 1,000 coins 10 times each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one, v_rand, v_min = run_experiment(1000, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4, 0.1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_one, v_rand, v_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a version that doesn't create silly objects--let's run the full experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5003f1b06ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mv_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-d88aaeeb34fc>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(num_coins, num_flips, num_runs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mflipped_coins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflip_coins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_coins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_flips\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get an array of coins\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# we have the direct values of frequency of heads so we can just get the v's.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-cfb8af3fedcb>\u001b[0m in \u001b[0;36mflip_coins\u001b[1;34m(num_coins, num_flips, thresh)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_flips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mnum_heads\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# else don't do anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "v_one, v_rand, v_min = run_experiment(1000, 10, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one, v_rand, v_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the error of $h$ in approximating $y$, where there is a probability of $\\lambda$ that $y = f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h$ approximates $f$ and makes an error with probability $\\mu$. \n",
    "We want the probability $h$ makes an error on $y$, so there are two main  cases:\n",
    "1. $h$ is a correct approximation, but $y \\neq f(x)$.\n",
    "2. $h$ is incorrect, and $y = f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking probabilities for each case:\n",
    "1. $(1-\\mu)(1-\\lambda)$\n",
    "2. $\\mu \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "From the above, the answer is $$(1 - \\lambda)(1 - \\mu) + \\lambda \\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__\n",
    "\n",
    "If the performance of $h$ is independent of $\\mu$, that means the performance doesn't depend on how closely it tracks $f$, meaning that $f$ must be completely different from the actual (noisy) target $y$. So we have $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_target_function():\n",
    "    \"\"\"\n",
    "    create target function by initializing a line passing thru two random points in R2.\n",
    "    \"\"\"\n",
    "    x1 = random.uniform(-1,1)\n",
    "    x2 = random.uniform(-1,1)\n",
    "    y1 = random.uniform(-1,1)\n",
    "    y2 = random.uniform(-1,1)\n",
    "    \n",
    "    target_function = (x1,y1,x2,y2)\n",
    "    \n",
    "    return target_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFunction(x1,y1,x2,y2,x3,y3):\n",
    "    u = (x2-x1)*(y3-y1) - (y2-y1)*(x3-x1)\n",
    "    if u >= 0:\n",
    "        return 1\n",
    "    elif u < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(size):\n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        x = np.random.uniform(-1,1,2) # generate (x,y)\n",
    "        x = np.insert(x,0,1)\n",
    "        dataset.append(x)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to write the linear regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll try to do the normal eqn thing to see how  it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = create_dataset(4)\n",
    "target_function = create_target_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.        ,  0.07728187,  0.77059638]),\n",
       " array([ 1.        ,  0.9786508 ,  0.72865224]),\n",
       " array([ 1.        ,  0.74650105,  0.49978432]),\n",
       " array([ 1.        , -0.35799172,  0.23422898])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1,y1,x2,y2 = target_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    a,b = x[1],x[2]\n",
    "    val = targetFunction(x1,y1,x2,y2,a,b)\n",
    "    y.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 1, -1]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2 points above, 2 below the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.linalg.pinv(X).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.239723  ,  2.19832849, -1.85121591])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This w indeed has the dimensionality we want. Now we want to confirm what happens when we take $w^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.07728187,  0.77059638])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_t = w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w_t.dot(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that the dimensions of $w$ are $(d+1) \\times N$, where $N$ is the number of training examples and $(d+1)$ is the number of dimensions in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_y_vals(dataset, target_function):\n",
    "    x1,y1,x2,y2 = target_function\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        a,b = x[1],x[2]\n",
    "        val = targetFunction(x1,y1,x2,y2,a,b)\n",
    "        y.append(val)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(dataset, target_function, debug=False):\n",
    "    \"\"\"\n",
    "    perform the linear regression algorithm, using the normal equation\n",
    "    \n",
    "    return: w, where w = Xt * y, where Xt is the pseudo-inverse of X.\n",
    "    \"\"\"\n",
    "    X = dataset\n",
    "    X = np.array(X)\n",
    "    x1,y1,x2,y2 = target_function # unpack\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    X_inv = np.linalg.pinv(X)\n",
    "        \n",
    "    w = X_inv.dot(y)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"pseudo-inv of X: \" + str(X_inv))\n",
    "        print(\"y: \" + str(y))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_sample_error(dataset, target_function, regression, debug=False):\n",
    "    \"\"\"\n",
    "    takes in a dataset and a regression output w, checks the in-sample error\n",
    "    \"\"\"\n",
    "    X = np.array(dataset)\n",
    "    N = len(dataset)\n",
    "    w = regression\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X is: \" + str(X))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    #error = err[0]\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def out_sample_error(num_points, target_function, regression):\n",
    "    dataset = create_dataset(num_points) # will be 1000 for problem 6.\n",
    "    X = np.array(dataset)\n",
    "    N = num_points\n",
    "    w = regression\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(dataset_size, num_times, num_test, debug=False):\n",
    "    \"\"\"\n",
    "    run a linear regression experiment, keeping the g functions and taking avg error\n",
    "    \n",
    "    num_test is # of testing pts\n",
    "    \"\"\"\n",
    "    dataset = create_dataset(dataset_size)\n",
    "    g = [] # keep track of results of linear regression\n",
    "    error_in = 0\n",
    "    error_out = 0\n",
    "    \n",
    "    for i in range(num_times):\n",
    "        target_function = create_target_function()\n",
    "        \n",
    "        w = linear_regression(dataset, target_function, debug=debug)\n",
    "        err = in_sample_error(dataset, target_function, w, debug=debug)\n",
    "        err_out = out_sample_error(num_test, target_function, w)\n",
    "        \n",
    "        g.append(w)\n",
    "        error_in += err\n",
    "        error_out += err_out\n",
    "        \n",
    "    error_in /= num_times\n",
    "    error_out /= num_times\n",
    "    \n",
    "    return g, error_in, error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run the experiment 1000 times on 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, error_in, error_out = run_experiment(100, 1000, 1000) # third arg is testing w/ 1000 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29791842722554635"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30405259006653285"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "Our error is 0.29, which is closest to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want an out-of-sample error, meaning that we need to generate 1000 random points to get the linear regression line's error on these. Recall that we need the g from before.\n",
    "\n",
    "I added code to calculate out-of-sample error, so we just use what we got above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30405259006653285"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "Out-of-sample error is about 0.3, which is closest to 0.5. Notice that this tracks E_in pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dis boi we need to add the PLA code w/ some mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFunction(x1,y1,x2,y2,x3,y3):\n",
    "    u = (x2-x1)*(y3-y1) - (y2-y1)*(x3-x1)\n",
    "    if u >= 0:\n",
    "        return 1\n",
    "    elif u < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misclassified(value, target_function, w):\n",
    "    \"\"\"\n",
    "    tells us if hypothesis output for a training example is correct or not, based on target function.\n",
    "    \"\"\"\n",
    "    (x1,y1,x2,y2) = target_function\n",
    "    x = value[1]\n",
    "    y = value[2]\n",
    "    \n",
    "    true_val = targetFunction(x1,y1,x2,y2,x,y)\n",
    "    hypothesis = np.sign(np.inner(w,value))\n",
    "    \n",
    "#     print(\"true val:\" + str(true_val))\n",
    "#     print(\"hypothesis:  \" + str(hypothesis))\n",
    "#     print('true val: '+ str(true_val))\n",
    "    \n",
    "    return not (hypothesis == true_val) # true if the value is indeed misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_misclassified(dataset, target_function, weight_vector):\n",
    "    (x1,y1,x2,y2) = target_function # unpack\n",
    "    misclassified_vals = []\n",
    "    for index in range(len(dataset)):\n",
    "        # print(\"value: \" + str(dataset[index]))\n",
    "        # hypothesis = np.sign(np.inner(weight_vector, dataset[index]))\n",
    "        # print(\"hypothesis: \" + str(hypothesis))\n",
    "        if misclassified(dataset[index], target_function, weight_vector):\n",
    "            misclassified_vals.append(dataset[index])\n",
    "            \n",
    "    return misclassified_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLA(dataset, target_function, regression, debug=False):\n",
    "    \"\"\"\n",
    "    the Perceptron Learning Algorithm, for full dataset\n",
    "    \"\"\"\n",
    "    #y_vals = eval_target_function(dataset, target_function) # get the array of y values for each point in dataset.\n",
    "    \n",
    "    (x1,y1,x2,y2) = target_function # unpack values\n",
    "    w = regression # init weight vector to output of Linear Regression. Note that this will be 3x1 since x_i in X has 3 features\n",
    "    num_iters = 0 # keep track of num iterations\n",
    "    \n",
    "    misclassified_vals = dataset\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        num_wrong = 0\n",
    "        for point in misclassified_vals:\n",
    "            x,y = point[1], point[2]\n",
    "            if np.sign(np.dot(w,point)) != targetFunction(x1, y1, x2, y2, x, y):\n",
    "                w = np.add(w, targetFunction(x1, y1, x2, y2, x, y) * point) # move w in right direction\n",
    "                num_wrong += 1\n",
    "                num_iters += 1\n",
    "                misclassified_vals = get_all_misclassified(dataset, target_function, w)\n",
    "                break\n",
    "\n",
    "        if num_wrong == 0:\n",
    "            done = True\n",
    "        \n",
    "    # now that nothing is misclassified, we get our hypothesis as a vector\n",
    "    g = []\n",
    "    f = []\n",
    "    for point in dataset:\n",
    "        x,y = point[1],point[2]\n",
    "        prediction = np.sign(np.dot(w,point))\n",
    "        true_val = targetFunction(x1,y1,x2,y2,x,y)\n",
    "        g.append(prediction)\n",
    "        f.append(true_val)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"g is: \" + str(g))\n",
    "        \n",
    "    # calcualte P[g != f] where f was the target function.\n",
    "    size = len(g)\n",
    "    num_wrong = 0\n",
    "    for i in range(size):\n",
    "        if debug:\n",
    "            print(\"i is: \" + str(i))\n",
    "        if g[i] != f[i]:\n",
    "            num_wrong += 1\n",
    "            \n",
    "    prob_different = num_wrong / size # this is the probability P[g != f] where g is the hypothesis values.\n",
    "    \n",
    "    return w, num_wrong, num_iters, prob_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLA_LR_experiment(dataset_size, num_times):\n",
    "    dataset = create_dataset(dataset_size)\n",
    "    avg_iters = 0\n",
    "    \n",
    "    for i in range(num_times):\n",
    "        target_function = create_target_function()\n",
    "        \n",
    "        w = linear_regression(dataset, target_function) # perform linear regression to get this vector boi\n",
    "        \n",
    "        w, num_wrong, num_iters, prob_different = PLA(dataset, target_function, w) # run PLA\n",
    "        \n",
    "        avg_iters += num_iters # we are interested in number of iterations.\n",
    "        \n",
    "    avg_iters /= num_times\n",
    "    \n",
    "    return avg_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.595"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PLA_LR_experiment(10, 1000) # run 1000 times w/ dataset of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[a]__\n",
    "\n",
    "I'm gonna hope nothing got messed up... Our avg number of iterations is 2.595 which is very close to 1 (answer __[a]__), so that's what we get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's add noise to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([3,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -6, -1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_dataset(dataset_size, prob_noise):\n",
    "    \"\"\"\n",
    "    create a dataset of a given size, with a certain probability that bits are flipped (noise)\n",
    "    \"\"\"\n",
    "    dataset = create_dataset(dataset_size) # create initial dataset\n",
    "    \n",
    "    num_affected = int(prob_noise * len(dataset)) # e.g. if we want noise in 10% of the dataset\n",
    "    \n",
    "    for i in range(num_affected):\n",
    "        idx = np.random.randint(0,dataset_size)\n",
    "        dataset[idx] = -dataset[idx]\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    x = random.choice(y)\n",
    "    x = -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_noisy(dataset, noise_factor=0.10, debug=False):\n",
    "    \"\"\"\n",
    "    perform the linear regression algorithm, using the normal equation\n",
    "    \n",
    "    return: w, where w = Xt * y, where Xt is the pseudo-inverse of X.\n",
    "    \"\"\"\n",
    "    X = dataset\n",
    "    X = np.array(X)\n",
    "    y = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        x_1,x_2 = x[1],x[2]\n",
    "        y_val = np.sign((x_1)**2 + (x_2)**2 - 0.6)\n",
    "        y.append(y_val)\n",
    "    \n",
    "    # add noise to y\n",
    "    num_affected = int(math.ceil(noise_factor * len(y)))\n",
    "    for i in range(num_affected):\n",
    "        index = random.randrange(0,len(y))\n",
    "        y[index] = -y[index]\n",
    "        \n",
    "    X_inv = np.linalg.pinv(X)\n",
    "        \n",
    "    w = X_inv.dot(y)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"pseudo-inv of X: \" + str(X_inv))\n",
    "        print(\"y: \" + str(y))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_sample_error_8(dataset, regression, debug=False):\n",
    "    \"\"\"\n",
    "    takes in a dataset and a regression output w, checks the in-sample error\n",
    "    \"\"\"\n",
    "    X = np.array(dataset)\n",
    "    N = len(dataset)\n",
    "    w = regression\n",
    "    y = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        x_1,x_2 = x[1],x[2]\n",
    "        y_val = np.sign((x_1)**2 + (x_2)**2 - 0.6)\n",
    "        y.append(y_val)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X is: \" + str(X))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    #error = err[0]\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy_experiment(dataset_size, prob_noise, num_times, num_test, out_sample=False, debug=False):\n",
    "    \"\"\"\n",
    "    same as last time, but now we want noisy dataset and are interested in E_in\n",
    "    \"\"\"\n",
    "    dataset = create_dataset(dataset_size)\n",
    "    g = [] # keep track of results of linear regression\n",
    "    error_in = 0\n",
    "    error_out = 0\n",
    "    \n",
    "    for i in range(num_times):\n",
    "        w = linear_regression_noisy(dataset, debug=debug)\n",
    "        err = in_sample_error_8(dataset, w, debug=debug)\n",
    "        \n",
    "        g.append(w)\n",
    "        error_in += err\n",
    "        \n",
    "        if out_sample:\n",
    "            err_out = out_sample_error(num_test, target_function, w)\n",
    "            error_out += err_out\n",
    "        \n",
    "    error_in /= num_times\n",
    "    \n",
    "    if out_sample:\n",
    "        error_out /= num_times\n",
    "        return g, error_in, error_out\n",
    "    \n",
    "    return g, error_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, error_in = noisy_experiment(1000, 0.1, 1000, 0) # 1000 points, 1000 times w/ 10% chance noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98985289862868264"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[c]__\n",
    "\n",
    "We get 0.368 for E_in, which is closest to 0.3. Notice that this is a somewhat higher in-sample error than when we run experiments that have no noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to transform the feature vectors $x = (1, x_1, x_2)$ into $$ x = (1, x_1, x_2, x_1x_2, x_1^2, x_2^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = create_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.26660633 -0.68398176]\n",
      "[ 1.          0.50420892  0.60013603]\n",
      "[ 1.         -0.05182251 -0.57265285]\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):\n",
    "    x = dataset[index]\n",
    "    x_1, x_2 = x[1], x[2]\n",
    "        \n",
    "    dataset[index] = (1, x_1, x_2, x_1*x_2, (x_1)**2, (x_2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, -0.2666063282541602, -0.68398176226172991, 0.18235386622940972, 0.071078934265165011, 0.46783105110666162)\n",
      "(1, 0.50420892218493996, 0.60013603439421237, 0.30259394306624987, 0.25422663721089883, 0.36016325977841124)\n",
      "(1, -0.051822513003188186, -0.57265285386778131, 0.029676309965875922, 0.0026855728539656085, 0.32793129104291452)\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_dataset(dataset):\n",
    "    \"\"\"\n",
    "    transform a dataset into the specification for prob 9.\n",
    "    \"\"\"\n",
    "    for index in range(len(dataset)):\n",
    "        x = dataset[index]\n",
    "        x_1, x_2 = x[1], x[2]\n",
    "        \n",
    "        dataset[index] = (1, x_1, x_2, x_1*x_2, (x_1)**2, (x_2)**2)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = create_dataset(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  0.97692497657082766,\n",
       "  -0.19240835400564826,\n",
       "  -0.18796852672899944,\n",
       "  0.95438240984791223,\n",
       "  0.037020974691162861),\n",
       " (1,\n",
       "  0.58971218549680127,\n",
       "  -0.78564266586962828,\n",
       "  -0.46330305350951168,\n",
       "  0.34776046172341374,\n",
       "  0.61723439843473638),\n",
       " (1,\n",
       "  -0.10476034715018523,\n",
       "  0.60904741911894211,\n",
       "  -0.063804019057824735,\n",
       "  0.010974730335027322,\n",
       "  0.37093875873544435),\n",
       " (1,\n",
       "  0.41271738337042452,\n",
       "  -0.9347771186229854,\n",
       "  -0.38579876643262345,\n",
       "  0.17033563853612996,\n",
       "  0.87380826150109092),\n",
       " (1,\n",
       "  0.60676295368030719,\n",
       "  0.1752915779955706,\n",
       "  0.10636043561987436,\n",
       "  0.36816128195885062,\n",
       "  0.030727137316177211),\n",
       " (1,\n",
       "  0.65940264411118488,\n",
       "  -0.016050574438287057,\n",
       "  -0.010583791224109882,\n",
       "  0.43481184706082193,\n",
       "  0.00025762093979899391),\n",
       " (1,\n",
       "  0.15387419585745654,\n",
       "  0.32618950382140199,\n",
       "  0.050192147597660983,\n",
       "  0.023677268150778898,\n",
       "  0.10639959240325243),\n",
       " (1,\n",
       "  0.31940744985330261,\n",
       "  0.030260171139249614,\n",
       "  0.0096653240957122258,\n",
       "  0.10202111902179002,\n",
       "  0.00091567795737667527),\n",
       " (1,\n",
       "  -0.3777480565155682,\n",
       "  0.97636975392396375,\n",
       "  -0.36882177698536089,\n",
       "  0.14269359420128891,\n",
       "  0.95329789637754148),\n",
       " (1,\n",
       "  0.94321139273948829,\n",
       "  -0.26670482216127156,\n",
       "  -0.2515590267610705,\n",
       "  0.88964773139356523,\n",
       "  0.071131462164075496)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = transform_dataset(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_function = create_target_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_tilde = linear_regression(dataset, target_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.47639423,  2.89487874,  0.08616445, -2.18163229, -3.08158489,\n",
       "        1.26410334])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need the value sign($\\tilde{w}^Tz$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(np.sign(w_tilde.dot(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_1(x_1, x_2):\n",
    "    return np.sign(-1 - 0.05*x_1 + 0.08*x_2 + 0.13*x_1*x_2 + 1.5*(x_1)**2 + 1.5*(x_2)**2)\n",
    "\n",
    "def g_2(x_1, x_2):\n",
    "    return np.sign(-1 - 0.05*x_1 + 0.08*x_2 + 0.13*x_1*x_2 + 1.5*(x_1)**2 + 15*(x_2)**2)\n",
    "\n",
    "def g_3(x_1, x_2):\n",
    "    return np.sign(-1 - 0.05*x_1 + 0.08*x_2 + 0.13*x_1*x_2 + 15*(x_1)**2 + 1.5*(x_2)**2)\n",
    "\n",
    "def g_4(x_1, x_2):\n",
    "    return np.sign(-1 - 1.5*x_1 + 0.08*x_2 + 0.13*x_1*x_2 + 0.05*(x_1)**2 + 0.05*(x_2)**2)\n",
    "\n",
    "def g_5(x_1, x_2):\n",
    "    return np.sign(-1 - 0.05*x_1 + 0.08*x_2 + 1.5*x_1*x_2 + 0.15*(x_1)**2 + 0.15*(x_2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5,  1. ,  1.5])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add([0,0,0], [1,2,3]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_target(dataset, debug=False):\n",
    "    \"\"\"\n",
    "    perform the linear regression algorithm, using the normal equation with a specified target function\n",
    "    \n",
    "    return: w, where w = Xt * y, where Xt is the pseudo-inverse of X.\n",
    "    \"\"\"\n",
    "    X = dataset\n",
    "    X = np.array(X)\n",
    "    y = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        x_1,x_2 = x[1],x[2]\n",
    "        y_val = np.sign((x_1)**2 + (x_2)**2 - 0.6)\n",
    "        y.append(y_val)\n",
    "        \n",
    "    X_inv = np.linalg.pinv(X)\n",
    "        \n",
    "    w = X_inv.dot(y)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"pseudo-inv of X: \" + str(X_inv))\n",
    "        print(\"y: \" + str(y))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def out_sample_error_target(num_points, regression, debug=False):\n",
    "    dataset_orig = create_dataset(num_points) # will be 1000 for problem 6.\n",
    "    dataset = transform_dataset(dataset_orig)\n",
    "    X = np.array(dataset)\n",
    "    N = num_points\n",
    "    w = regression\n",
    "    y = []\n",
    "    \n",
    "    for x in dataset_orig:\n",
    "        x_1,x_2 = x[1],x[2]\n",
    "        y_val = np.sign((x_1)**2 + (x_2)**2 - 0.6)\n",
    "        y.append(y_val)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X shape: \" + str(X.shape))\n",
    "        print(\"y shape: \" + str(y.shape))\n",
    "        print(\"X: \" + str(X))\n",
    "        print(\"y: \" + str(y))\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformed_LR_experiment(dataset_size, num_runs, debug=False):\n",
    "    \"\"\"\n",
    "    note that here we are interested in w_tilde, the solution for the transformed, nonlinear feature vectors\n",
    "    we will use N=1000 for question 9.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_orig = create_dataset(dataset_size)\n",
    "    dataset = transform_dataset(dataset_orig)\n",
    "    \n",
    "    avg_agreements = [0, 0, 0, 0, 0]\n",
    "    error_out = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        w_tilde = linear_regression_target(dataset)\n",
    "        g_agreements = [0, 0, 0, 0, 0] # b/c we have 5 hypotheses atm\n",
    "        err_out = out_sample_error_target(1000, w_tilde, debug=False)\n",
    "        \n",
    "        for i in range(dataset_size):\n",
    "            x_g = []\n",
    "            x_orig = dataset_orig[i]\n",
    "            x_1,x_2 = x_orig[1],x_orig[2]\n",
    "            x_g.append(g_1(x_1,x_2))\n",
    "            x_g.append(g_2(x_1,x_2))\n",
    "            x_g.append(g_3(x_1,x_2))\n",
    "            x_g.append(g_4(x_1,x_2))\n",
    "            x_g.append(g_5(x_1,x_2))\n",
    "            \n",
    "            prediction = np.sign(w_tilde.dot(x))\n",
    "            if debug:\n",
    "                print(\"w_tilde: \" + str(w_tilde))\n",
    "                print(\"x: \" +  str(x))\n",
    "                print(\"prediction: \" + str(prediction))\n",
    "            for g in x_g:\n",
    "                if prediction == g:\n",
    "                    g_agreements[x_g.index(g)] += 1 # increment number of agreements at that index--e.g. 0 for g_1, etc.\n",
    "                    \n",
    "        avg_agreements = np.add(avg_agreements, g_agreements)\n",
    "        error_out += err_out\n",
    "        \n",
    "    #avg_agreements /= num_runs\n",
    "    avg_agreements = np.true_divide(avg_agreements, num_runs, casting='unsafe')\n",
    "    error_out /= num_runs\n",
    "    \n",
    "    return avg_agreements, error_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_tilde: [-1.26274003  0.0158619  -0.00414948  0.12275936  1.96951405  2.00993288]\n",
      "x: -1\n",
      "prediction: [ 1. -1.  1. -1. -1. -1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-a856a18d7797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mavg_agreements\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformed_LR_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we'll run 20 times w/ N = 1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-226-792d493e8e4e>\u001b[0m in \u001b[0;36mtransformed_LR_experiment\u001b[1;34m(dataset_size, num_runs, debug)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"prediction: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_g\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                     \u001b[0mg_agreements\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# increment number of agreements at that index--e.g. 0 for g_1, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "avg_agreements, error_out = transformed_LR_experiment(1000, 20, debug=True) # we'll run 20 times w/ N = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[a]__\n",
    "\n",
    "Looks like the first hypothesis wins by far, with 1670 agreements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_sample_error()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
