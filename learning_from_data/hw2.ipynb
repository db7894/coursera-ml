{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1000\"\n",
       "            src=\"hw2.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1cdd9e3a400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"hw2.pdf\", width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Coin:\n",
    "    \"\"\"\n",
    "    a simple coin class that gives us a utility to flip a single coin a given number of times (fair or unfair).\n",
    "    \"\"\"\n",
    "    def __init__(self, side=None):\n",
    "        self.side = side\n",
    "        self.heads_flipped = 0 # number of heads flipped in this coin's history\n",
    "        self.tails_flipped = 0\n",
    "        self.frac_heads = 0 # fraction of heads depending on num_flips\n",
    "    def flip(self, thresh=0.5, num_flips=1, track_flips=False):\n",
    "        \"\"\"\n",
    "        flip the coin, 50% probability of self.side changing to heads/tails or whatever thresh is\n",
    "        \"\"\"\n",
    "        self.heads_flipped = 0 # reset every time we do a number of flips.\n",
    "        self.tails_flipped = 0\n",
    "        \n",
    "        num_heads = 0\n",
    "        num_tails = 0\n",
    "        \n",
    "        if track_flips:\n",
    "            flips_so_far = []\n",
    "            \n",
    "        for i in range(num_flips):\n",
    "            val = np.random.uniform(0,1)\n",
    "            if val >= 0.5:\n",
    "                self.side = \"heads\"\n",
    "                self.heads_flipped += 1\n",
    "                num_heads += 1\n",
    "            else:\n",
    "                self.side = \"tails\"\n",
    "                self.tails_flipped += 1\n",
    "                num_tails += 1\n",
    "                \n",
    "            # print(\"Flipped \" + str(self.side))\n",
    "                \n",
    "            if track_flips:\n",
    "                flips_so_far.append(self.side) #append this flip\n",
    "        \n",
    "        self.frac_heads = self.heads_flipped / num_flips # store this fraction.\n",
    "        \n",
    "        if track_flips:\n",
    "            return num_heads, num_tails, flips_so_far\n",
    "        \n",
    "        return num_heads, num_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Coin()\n",
    "heads, tails = c.flip(num_flips=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_coin_objects(num_coins, num_flips):\n",
    "    \"\"\"\n",
    "    simulates the flipping of a number of coins--flips num_coins coins, flipping each for num_flips flips\n",
    "    \"\"\"\n",
    "    coins = []\n",
    "    for i in range(num_coins):\n",
    "        c = Coin()\n",
    "        c.flip(num_flips = num_flips)\n",
    "        coins.append(c) # so we can retrieve properties of individual coins.\n",
    "        # heads, tails = c.heads_flipped, c.tails_flipped # get number of heads/tails\n",
    "        \n",
    "    return coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flipped_coins = flip_coins(5,10) # test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = flipped_coins[0]\n",
    "c.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = min(flipped_coins, key=attrgetter('heads_flipped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.heads_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for coin in flipped_coins:\n",
    "    print(coin.heads_flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so this appears to work--we have flip_coins which will create an array of coins which have properties we can access. Now we need a function to run the full experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_coins(num_coins, num_flips, thresh=0.5):\n",
    "    head_freqs = []\n",
    "    for i in range(num_coins):\n",
    "        num_heads = 0\n",
    "        \n",
    "        for j in range(num_flips):\n",
    "            val = np.random.uniform(0,1)\n",
    "            if val >= 0.5:\n",
    "                num_heads += 1 # else don't do anything.\n",
    "            \n",
    "        heads_frac = num_heads / num_flips\n",
    "        head_freqs.append(heads_frac)\n",
    "        \n",
    "    return head_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(num_coins, num_flips, num_runs):\n",
    "    \"\"\"\n",
    "    runs an experiment of flipping a certain number of coins\n",
    "    \"\"\"\n",
    "    v_one_avg = 0\n",
    "    v_rand_avg = 0\n",
    "    v_min_avg = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        flipped_coins = flip_coins(num_coins, num_flips) # get an array of coins\n",
    "        \n",
    "        # we have the direct values of frequency of heads so we can just get the v's.\n",
    "        v_one = flipped_coins[0] # first coin\n",
    "        v_rand = random.choice(flipped_coins) # random coin\n",
    "        v_min = min(flipped_coins) # coin w/ min number heads flipped\n",
    "        \n",
    "        v_one_avg += v_one\n",
    "        v_rand_avg += v_rand\n",
    "        v_min_avg += v_min\n",
    "        \n",
    "    v_one_avg /= num_runs\n",
    "    v_rand_avg /= num_runs\n",
    "    v_min_avg /= num_runs\n",
    "    \n",
    "    return v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment_object(num_coins, num_flips, num_runs):\n",
    "    \"\"\"\n",
    "    runs an experiment of flipping a certain number of coins (object version)\n",
    "    \"\"\"\n",
    "    v_one_avg = 0\n",
    "    v_rand_avg = 0\n",
    "    v_min_avg = 0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        flipped_coins = flip_coins(num_coins, num_flips) # get an array of coins\n",
    "        \n",
    "        c_one = flipped_coins[0] # first coin\n",
    "        c_rand = random.choice(flipped_coins) # random coin\n",
    "        c_min = min(flipped_coins, key=attrgetter('heads_flipped')) # coin w/ min number heads flipped\n",
    "        \n",
    "        v_one = c_one.frac_heads\n",
    "        v_rand = c_rand.frac_heads\n",
    "        v_min = c_min.frac_heads\n",
    "        \n",
    "        v_one_avg += v_one\n",
    "        v_rand_avg += v_rand\n",
    "        v_min_avg += v_min\n",
    "        \n",
    "    v_one_avg /= num_runs\n",
    "    v_rand_avg /= num_runs\n",
    "    v_min_avg /= num_runs\n",
    "    \n",
    "    return v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the experiment, and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one_avg, v_rand_avg, v_min_avg = run_experiment(5,10,1) # test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.5, 0.4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_one_avg, v_rand_avg, v_min_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run 100,000 times, flipping 1,000 coins 10 times each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one, v_rand, v_min = run_experiment(1000, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4, 0.1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_one, v_rand, v_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a version that doesn't create silly objects--let's run the full experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5003f1b06ad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mv_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_rand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-d88aaeeb34fc>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(num_coins, num_flips, num_runs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mflipped_coins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflip_coins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_coins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_flips\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# get an array of coins\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# we have the direct values of frequency of heads so we can just get the v's.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-cfb8af3fedcb>\u001b[0m in \u001b[0;36mflip_coins\u001b[1;34m(num_coins, num_flips, thresh)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_flips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mnum_heads\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# else don't do anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "v_one, v_rand, v_min = run_experiment(1000, 10, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_one, v_rand, v_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the error of $h$ in approximating $y$, where there is a probability of $\\lambda$ that $y = f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h$ approximates $f$ and makes an error with probability $\\mu$. \n",
    "We want the probability $h$ makes an error on $y$, so there are two main  cases:\n",
    "1. $h$ is a correct approximation, but $y \\neq f(x)$.\n",
    "2. $h$ is incorrect, and $y = f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking probabilities for each case:\n",
    "1. $(1-\\mu)(1-\\lambda)$\n",
    "2. $\\mu \\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "From the above, the answer is $$(1 - \\lambda)(1 - \\mu) + \\lambda \\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[d]__\n",
    "\n",
    "If the performance of $h$ is independent of $\\mu$, that means the performance doesn't depend on how closely it tracks $f$, meaning that $f$ must be completely different from the actual (noisy) target $y$. So we have $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_target_function():\n",
    "    \"\"\"\n",
    "    create target function by initializing a line passing thru two random points in R2.\n",
    "    \"\"\"\n",
    "    x1 = random.uniform(-1,1)\n",
    "    x2 = random.uniform(-1,1)\n",
    "    y1 = random.uniform(-1,1)\n",
    "    y2 = random.uniform(-1,1)\n",
    "    \n",
    "    target_function = (x1,y1,x2,y2)\n",
    "    \n",
    "    return target_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFunction(x1,y1,x2,y2,x3,y3):\n",
    "    u = (x2-x1)*(y3-y1) - (y2-y1)*(x3-x1)\n",
    "    if u >= 0:\n",
    "        return 1\n",
    "    elif u < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(size):\n",
    "    dataset = []\n",
    "    for i in range(size):\n",
    "        x = np.random.uniform(-1,1,2) # generate (x,y)\n",
    "        x = np.insert(x,0,1)\n",
    "        dataset.append(x)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to write the linear regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll try to do the normal eqn thing to see how  it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = create_dataset(4)\n",
    "target_function = create_target_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.        ,  0.07728187,  0.77059638]),\n",
       " array([ 1.        ,  0.9786508 ,  0.72865224]),\n",
       " array([ 1.        ,  0.74650105,  0.49978432]),\n",
       " array([ 1.        , -0.35799172,  0.23422898])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1,y1,x2,y2 = target_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    a,b = x[1],x[2]\n",
    "    val = targetFunction(x1,y1,x2,y2,a,b)\n",
    "    y.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 1, -1]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2 points above, 2 below the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.linalg.pinv(X).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.239723  ,  2.19832849, -1.85121591])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This w indeed has the dimensionality we want. Now we want to confirm what happens when we take $w^Tx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.07728187,  0.77059638])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_t = w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w_t.dot(x)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that the dimensions of $w$ are $(d+1) \\times N$, where $N$ is the number of training examples and $(d+1)$ is the number of dimensions in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_y_vals(dataset, target_function):\n",
    "    x1,y1,x2,y2 = target_function\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        a,b = x[1],x[2]\n",
    "        val = targetFunction(x1,y1,x2,y2,a,b)\n",
    "        y.append(val)\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(dataset, target_function, debug=False):\n",
    "    \"\"\"\n",
    "    perform the linear regression algorithm, using the normal equation\n",
    "    \n",
    "    return: w, where w = Xt * y, where Xt is the pseudo-inverse of X.\n",
    "    \"\"\"\n",
    "    X = dataset\n",
    "    X = np.array(X)\n",
    "    x1,y1,x2,y2 = target_function # unpack\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    X_inv = np.linalg.pinv(X)\n",
    "        \n",
    "    w = X_inv.dot(y)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"pseudo-inv of X: \" + str(X_inv))\n",
    "        print(\"y: \" + str(y))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_sample_error(dataset, target_function, regression, debug=False):\n",
    "    \"\"\"\n",
    "    takes in a dataset and a regression output w, checks the in-sample error\n",
    "    \"\"\"\n",
    "    X = np.array(dataset)\n",
    "    N = len(dataset)\n",
    "    w = regression\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X is: \" + str(X))\n",
    "        print(\"w: \" + str(w))\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    #error = err[0]\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def out_sample_error(num_points, target_function, regression):\n",
    "    dataset = create_dataset(num_points) # will be 1000 for problem 6.\n",
    "    X = np.array(dataset)\n",
    "    N = num_points\n",
    "    w = regression\n",
    "    y = create_y_vals(dataset,target_function)\n",
    "    \n",
    "    m = X.dot(w) - y\n",
    "    \n",
    "    err = 1/N * ((m.T).dot(m))\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(dataset_size, num_times, num_test, debug=False):\n",
    "    \"\"\"\n",
    "    run a linear regression experiment, keeping the g functions and taking avg error\n",
    "    \n",
    "    num_test is # of testing pts\n",
    "    \"\"\"\n",
    "    dataset = create_dataset(dataset_size)\n",
    "    g = [] # keep track of results of linear regression\n",
    "    error_in = 0\n",
    "    error_out = 0\n",
    "    \n",
    "    for i in range(num_times):\n",
    "        target_function = create_target_function()\n",
    "        \n",
    "        w = linear_regression(dataset, target_function, debug=debug)\n",
    "        err = in_sample_error(dataset, target_function, w, debug=debug)\n",
    "        err_out = out_sample_error(num_test, target_function, w)\n",
    "        \n",
    "        g.append(w)\n",
    "        error_in += err\n",
    "        error_out += err_out\n",
    "        \n",
    "    error_in /= num_times\n",
    "    error_out /= num_times\n",
    "    \n",
    "    return g, error_in, error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run the experiment 1000 times on 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, error_in, error_out = run_experiment(100, 1000, 1000) # third arg is testing w/ 1000 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29791842722554635"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30405259006653285"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "Our error is 0.29, which is closest to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want an out-of-sample error, meaning that we need to generate 1000 random points to get the linear regression line's error on these. Recall that we need the g from before.\n",
    "\n",
    "I added code to calculate out-of-sample error, so we just use what we got above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30405259006653285"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[e]__\n",
    "\n",
    "Out-of-sample error is about 0.3, which is closest to 0.5. Notice that this tracks E_in pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dis boi we need to add the PLA code w/ some mods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def targetFunction(x1,y1,x2,y2,x3,y3):\n",
    "    u = (x2-x1)*(y3-y1) - (y2-y1)*(x3-x1)\n",
    "    if u >= 0:\n",
    "        return 1\n",
    "    elif u < 0:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misclassified(value, target_function, w):\n",
    "    \"\"\"\n",
    "    tells us if hypothesis output for a training example is correct or not, based on target function.\n",
    "    \"\"\"\n",
    "    (x1,y1,x2,y2) = target_function\n",
    "    x = value[1]\n",
    "    y = value[2]\n",
    "    \n",
    "    true_val = targetFunction(x1,y1,x2,y2,x,y)\n",
    "    hypothesis = np.sign(np.inner(w,value))\n",
    "    \n",
    "#     print(\"true val:\" + str(true_val))\n",
    "#     print(\"hypothesis:  \" + str(hypothesis))\n",
    "#     print('true val: '+ str(true_val))\n",
    "    \n",
    "    return not (hypothesis == true_val) # true if the value is indeed misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_misclassified(dataset, target_function, weight_vector):\n",
    "    (x1,y1,x2,y2) = target_function # unpack\n",
    "    misclassified_vals = []\n",
    "    for index in range(len(dataset)):\n",
    "        # print(\"value: \" + str(dataset[index]))\n",
    "        # hypothesis = np.sign(np.inner(weight_vector, dataset[index]))\n",
    "        # print(\"hypothesis: \" + str(hypothesis))\n",
    "        if misclassified(dataset[index], target_function, weight_vector):\n",
    "            misclassified_vals.append(dataset[index])\n",
    "            \n",
    "    return misclassified_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLA(dataset, target_function, regression, debug=False):\n",
    "    \"\"\"\n",
    "    the Perceptron Learning Algorithm, for full dataset\n",
    "    \"\"\"\n",
    "    #y_vals = eval_target_function(dataset, target_function) # get the array of y values for each point in dataset.\n",
    "    \n",
    "    (x1,y1,x2,y2) = target_function # unpack values\n",
    "    w = regression # init weight vector to output of Linear Regression. Note that this will be 3x1 since x_i in X has 3 features\n",
    "    num_iters = 0 # keep track of num iterations\n",
    "    \n",
    "    misclassified_vals = dataset\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        num_wrong = 0\n",
    "        for point in misclassified_vals:\n",
    "            x,y = point[1], point[2]\n",
    "            if np.sign(np.dot(w,point)) != targetFunction(x1, y1, x2, y2, x, y):\n",
    "                w = np.add(w, targetFunction(x1, y1, x2, y2, x, y) * point) # move w in right direction\n",
    "                num_wrong += 1\n",
    "                num_iters += 1\n",
    "                misclassified_vals = get_all_misclassified(dataset, target_function, w)\n",
    "                break\n",
    "\n",
    "        if num_wrong == 0:\n",
    "            done = True\n",
    "        \n",
    "    # now that nothing is misclassified, we get our hypothesis as a vector\n",
    "    g = []\n",
    "    f = []\n",
    "    for point in dataset:\n",
    "        x,y = point[1],point[2]\n",
    "        prediction = np.sign(np.dot(w,point))\n",
    "        true_val = targetFunction(x1,y1,x2,y2,x,y)\n",
    "        g.append(prediction)\n",
    "        f.append(true_val)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"g is: \" + str(g))\n",
    "        \n",
    "    # calcualte P[g != f] where f was the target function.\n",
    "    size = len(g)\n",
    "    num_wrong = 0\n",
    "    for i in range(size):\n",
    "        if debug:\n",
    "            print(\"i is: \" + str(i))\n",
    "        if g[i] != f[i]:\n",
    "            num_wrong += 1\n",
    "            \n",
    "    prob_different = num_wrong / size # this is the probability P[g != f] where g is the hypothesis values.\n",
    "    \n",
    "    return w, num_wrong, num_iters, prob_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PLA_LR_experiment(dataset_size, num_times):\n",
    "    dataset = create_dataset(dataset_size)\n",
    "    avg_iters = 0\n",
    "    \n",
    "    for i in range(num_times):\n",
    "        target_function = create_target_function()\n",
    "        \n",
    "        w = linear_regression(dataset, target_function) # perform linear regression to get this vector boi\n",
    "        \n",
    "        w, num_wrong, num_iters, prob_different = PLA(dataset, target_function, w) # run PLA\n",
    "        \n",
    "        avg_iters += num_iters # we are interested in number of iterations.\n",
    "        \n",
    "    avg_iters /= num_times\n",
    "    \n",
    "    return avg_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_all_misclassified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-132-0d44f5770cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPLA_LR_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# run 1000 times w/ dataset of size 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-131-d83174250be5>\u001b[0m in \u001b[0;36mPLA_LR_experiment\u001b[1;34m(dataset_size, num_times)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# perform linear regression to get this vector boi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_wrong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_different\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPLA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# run PLA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mavg_iters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_iters\u001b[0m \u001b[1;31m# we are interested in number of iterations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-8d82abca9dfc>\u001b[0m in \u001b[0;36mPLA\u001b[1;34m(dataset, target_function, regression, debug)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mnum_wrong\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mnum_iters\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mmisclassified_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_all_misclassified\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_all_misclassified' is not defined"
     ]
    }
   ],
   "source": [
    "PLA_LR_experiment(10, 1000) # run 1000 times w/ dataset of size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
